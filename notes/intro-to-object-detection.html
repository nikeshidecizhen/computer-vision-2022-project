<!doctype html>
<html>

<head>
    <title>Computer Vision 2022 Project</title>
    <link rel="icon" type="img/icon.svg" href="img/icon.svg">
    <meta charset="utf-8" name="viewport" content="width=device-width, initial-scale=1">
    <link href="../css/frame.css" media="screen" rel="stylesheet" type="text/css" />
    <link href="../css/controls.css" media="screen" rel="stylesheet" type="text/css" />
    <link href="../css/custom.css" media="screen" rel="stylesheet" type="text/css" />
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Open+Sans+Condensed:300,700' rel='stylesheet' type='text/css'>
    <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700" rel="stylesheet">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
    <script src="../js/menu.js"></script>
    <style>
        .menu-index {
            color: rgb(255, 255, 255) !important;
            opacity: 1 !important;
            font-weight: 700 !important;
        }
    </style>
</head>

<body>
    <div class="menu-container"></div>
    <div class="content-container">
        <div class="content">
            <div class="content-table flex-column">
                <!-------------------------------------------------------------------------------------------->
                <!--Start Intro-->
                <div class="flex-row">
                    <div class="flex-item flex-column">
                        <h2 class="add-top-margin">Introduction to Object Detection</h2>
                        <hr>
                        <p class="text">
                            目标检测 <strong>(Object
                                Detection)</strong> 的任务是找出图像中所有特定的目标，确定它们的类别和位置。由于各类物体有不同的外观和形状，加上成像时光照、遮挡等因素的干扰，目标检测一直是
                            CV
                            领域内具有挑战性的问题。 <br>
                            <br>
                            计算机视觉中关于图像识别有四大类任务：
                            <li>
                                <strong>Classification</strong>：给定一张图片或一段视频判断里面包含什么类别的目标。
                            </li>
                            <li>
                                <strong>Location</strong>：定位出这个目标的的位置。
                            </li>
                            <li>
                                <strong>Detection</strong>：定位出这个目标的位置并且知道目标物是什么。
                            </li>
                            <li>
                                <strong>Segmentation</strong>：分为实例的分割 (<strong>Instance-level</strong>) 和场景分割
                                (<strong>Scene-level</strong>)，解决“每一个像素属于哪个目标物或场景”的问题。
                        </p>
                        </li>
                        <div align=center>
                            <img src="/img/20220112222127.png" width="700px" />
                        </div>
                        <h2 class="add-top-margin">Algorithm</h2>
                        <p>基于深度学习的目标检测算法主要分为两类：Two stage 和 One stage。</p>
                        <li>
                            <strong>Tow Stage</strong>
                            <p>先进行区域生成，该区域称之为 <strong>region proposal</strong> (简称
                                <strong>RP</strong>，一个有可能包含待检物体的预选框)，再通过卷积神经网络进行样本分类。
                            </p>
                        </li>
                        <div align=center>
                            <img src="/img/20220304210002.png" width="600px" />
                        </div>
                        常见 tow stage 算法有：R-CNN、SPP-Net、Fast R-CNN、Faster R-CNN 和 R-FCN 等。
                        <br>
                        <br>
                        <li>
                            <strong>One Stage</strong>
                            <p>不用 RP，直接在网络中提取特征来预测物体分类和位置。</p>
                        </li>
                        <div align=center>
                            <img src="/img/20220304210148.png" width="400px" />
                        </div>

                        常见的 one stage 算法有：OverFeat、YOLOv1、YOLOv2、YOLOv3、SSD 和 RetinaNet 等。
                        <h2 class="add-top-margin">Theory</h2>
                        <p>目标检测分为两大系列—— <strong>RCNN</strong> 和 <strong>YOLO</strong>，RCNN
                            系列是基于<strong>区域检测</strong>的代表性算法，YOLO 是基于<strong>区域提取</strong>的代表性算法，另外还有著名的
                            <strong>SSD</strong> 是基于前两个系列的改进。
                        </p>
                        <h3 class="add-top-margin">Bounding Boxes</h3>
                        <p>很多目标检测技术都会涉及 <strong>候选框(bounding boxes)</strong>
                            的生成，物体候选框获取当前主要使用图像分割与区域生长技术。区域生长(合并)主要由于检测图像中存在的物体具有局部区域相似性(颜色、纹理等)。目标识别与图像分割技术的发展进一步推动有效提取图像中信息。
                        </p>

                        <li>
                            <strong>Sliding Window</strong>
                            <p>首先对输入图像进行不同窗口大小的滑窗进行从左往右、从上到下的滑动。每次滑动时候对当前窗口执行分类器(分类器是事先训练好的)。如果当前窗口得到较高的分类概率，则认为检测到了物体。对每个不同窗口大小的滑窗都进行检测后，会得到不同窗口检测到的物体标记，这些窗口大小会存在重复较高的部分，最后采用<strong>非极大值抑制</strong>(<strong>Non-Maximum
                                    Suppression, NMS</strong>)的方法进行筛选。最终，经过 NMS
                                筛选后获得检测到的物体。滑窗法简单易于理解，但是<strong>不同窗口大小进行图像全局搜索导致效率低下，而且设计窗口大小时候还需要考虑物体的长宽比</strong>。所以，对于<strong>实时性要求较高</strong>的分类器，不推荐使用滑窗法。
                            </p>
                        </li>
                        <li>
                            <strong>Selective Search(SS)</strong>
                            <p>滑窗法类似穷举进行图像子区域搜索，但是一般情况下图像中大部分子区域是没有物体的。学者们自然而然想到只对图像中最有可能包含物体的区域进行搜索以此来提高计算效率。<strong>选择搜索方法是当下最为熟知的图像
                                    bounding boxes 提取算法</strong>，由 Koen E.A 于2011年提出。</p>
                            <p>主要思想：图像中物体可能存在的区域应该是有某些<strong>相似性或者连续性</strong>区域的。因此，选择搜索基于上面这一想法采用<strong>子区域合并</strong>的方法进行提取
                                bounding
                                boxes。首先，对输入图像进行分割算法产生许多小的子区域。其次，根据这些子区域之间相似性(相似性标准主要有颜色、纹理、大小等等)进行区域合并，不断的进行区域迭代合并。每次迭代过程中对这些合并的子区域做
                                bounding boxes(外切矩形)，这些子区域外切矩形就是通常所说的候选框。</p>

                        <li><strong>step0</strong>：生成区域集 R</li>
                        <li><strong>step1</strong>：计算区域集 R 里每个相邻区域的相似度 S={s1, s2,…}</li>
                        <li><strong>step2</strong>：找出相似度最高的两个区域，将其合并为新集，添加进 R</li>
                        <li><strong>step3</strong>：从 S 中移除所有与 <strong>step2</strong>
                            中有关的子集</li>
                        <li><strong>step4</strong>：计算新集与所有子集的相似度</li>
                        <li><strong>step5</strong>：跳至 <strong>step2</strong>，直至 S 为空
                        </li>

                        </li>

                        <p>我们可以把目标标签定义如下：</p>
                        <img class="image center" src="http://latex.codecogs.com/svg.latex?y=\left[
                            \begin{matrix}
                            \quad p_c\quad b_x\quad
                            b_y\quad
                            b_h\quad
                            b_w\quad
                            c_1\quad
                            c_2\quad
                            c_3\quad
                            \end{matrix}
                            \right]^{T}" style="max-width: 400px;">
                        它是一个向量，pc
                        表示是否含有对象，如果存在，则 pc=1，如果是背景，则图片中没有要检测的对象，则 pc=0。我们可这样理解
                        pc，它表示被检测对象属于某一分类的概率，背景分类除外。如果检测到对象，就输出被检测对象的边界框参数 bx、by、bh 和
                        bw。最后，如果存在某对象，则让 pc=1，同时输出属于某个类别的概率 c1，c2，c3。</p>
                        <h3 class="add-top-margin">IoU</h3>
                        <p>使用 <strong>IoU(Intersection over Union)</strong> 来判断模型的好坏。
                            交并比函数做的是计算两个边界框交集和并集之比。两个边界框的并集是这个区域，就是属于包含两个边界框区域，而交集就是比较小的区域，那么交并比就是交集的大小，再除以并集面积。
                        </p>
                        <img class="image center"
                            src="http://latex.codecogs.com/svg.latex?\text{IoU}=\frac{\text{size of intersection}}{\text{size of union}}"
                            style="max-width: 200px;">


                        <p>一般约定，如果 IoU 小于 0.5，我们就可以说检测正确。但是 0.5 并不严格，如果想更加严格一些，可以把阀值从 0.5 改为 0.6 或者 0.7。
                        </p>

                        <h3 class="add-top-margin">NMS</h3>
                        <p>预测结果中，可能多个预测结果间存在重叠部分，需要保留交并比最大的、去掉非最大的预测结果，这就是非极大值抑制 <strong>(Non-Maximum
                                Suppression,NMS)</strong>。当有许多框框重合都表示他们中间存在检测结果时，我们必须要选择一个最好的。简单来说，就是选局部最大值。步骤如下：
                        </p>

                        <li>Discard all boxes with <strong> pc < 0.6</strong>
                        </li>
                        <li>While there are any remaining boxes:
                            <ul>
                                <li>Pick the box with the largest pc output that as a prediction.</li>
                                <li>Discard any remaining box with <strong>IoU > 0.5</strong> with the box output in
                                    the
                                    previous step. (为了不影响到另外的检测结果）</li>
                            </ul>
                        </li>

                        <br>

                        以上是算法检测单个对象的情况，如果尝试同时检测三个对象，
                        比如说行人、汽车、摩托，那么输出向量就会有三个额外的分量。事实证明，正确的做法是独立进行三次非极大值抑制，对每个输出类别都做一次。

                        <h3 class="add-top-margin">Anchor Boxes</h3>

                        <p>到目前为止，对象检测中存在的一个问题是每个格子只能检测出一个对象，如果想让一个格子检测出多个对象，可以使用 <strong>anchor
                                box</strong> 这个概念。</p>

                        <p>假设有这样一张图片，我们继续使用 3x3 网格，注意行人的中点和汽车的中点几乎在同一个地方，两者都落入到同一个格子中。而 anchor box
                            的思路是，这样，预先定义两个不同形状的 anchor box，或者 anchor box 形状，要做的是把预测结果和这两个 anchor box 关联起来。一般来说，可能会用更多的
                            anchor box，可能要 5 个甚至更多。</p>
                        <p>定义类别标签，用的向量不再是之前的：</p>
                        <img class="image center" src="http://latex.codecogs.com/svg.latex?y=\left[
                            \begin{matrix}
                            \quad p_c\quad b_x\quad
                            b_y\quad
                            b_h\quad
                            b_w\quad
                            c_1\quad
                            c_2\quad
                            c_3\quad
                            \end{matrix}
                            \right]^{T}" style="max-width: 400px;">

                        <img class="image center" src="http://latex.codecogs.com/svg.latex?y=\left[\quad
                            p_c\quad
                            b_x\quad
                            b_y\quad
                            b_h\quad
                            b_w\quad
                            c_1\quad
                            c_2\quad
                            c_3\quad
                            p_c\quad
                            b_x\quad
                            b_y\quad
                            b_h\quad
                            b_w\quad
                            c_1\quad
                            c_2\quad
                            c_3\quad
                            \right]^{T}" style="max-width: 700px;">
                        而是：


                        <p><strong>Comparing with the previous one</strong></p>

                        <li>
                            <strong>Previous</strong>
                            <p>训练图像中的每个对象都分配给包含该对象中点的网格单元。</p>
                        </li>
                        <li>
                            <strong>With two anchor boxes</strong>
                            <p>训练图像中的每个对象都分配给包含该对象的中点的网格单元和具有最高 IoU 的网格单元的锚框。</p>
                        </li>

                        <p>如果有两个 anchor
                            box，但在同一个格子中有三个对象，这种情况算法处理不好，希望这种情况不会发生，但如果真的发生了，这个算法并没有很好的处理办法，对于这种情况，就引入一些打破僵局的默认手段。还有这种情况，两个对象都分配到一个格子中，而且它们的
                            anchor box
                            形状也一样，这是算法处理不好的另一种情况，需要引入一些打破僵局的默认手段，专门处理这种情况，希望的数据集里不会出现这种情况，其实出现的情况不多，所以对性能的影响应该不会很大。</p>

                        <p>也许设立 anchor box 的好处在于能让学习算法能够更有针对性，特别是如果数据集有一些很高很瘦的对象，比如说行人，还有像汽车这样很宽的对象，这样算法就能更有针对性的处理。
                        </p>

                        <p>最后，应该怎么选择 anchor box 呢？人们一般手工指定 anchor box 形状，可以选 5 到 10 个 anchor box
                            形状，覆盖到多种不同的形状，可以涵盖想要检测的对象的各种形状。</p>
                        <p>还有一个更高级的版本，就是所谓的 <strong>K-means</strong>，可以将两类对象形状聚类，如果用它来选择一组 anchor
                            box，选择最具有代表性的一组 anchor box，可以代表试图检测的十几个对象类别，但这其实是自动选择 anchor box
                            的高级方法。如果就人工选择一些形状，合理的考虑到所有对象的形状，预计会检测的很高很瘦或者很宽很胖的对象。</p>
                        <h1 class="add-top-margin">Reference</h1>

                        <li><a href="https://blog.csdn.net/yegeli/article/details/109861867">目标检测（Object
                                Detection）</a></li>


                    </div>
                </div>
</body>

</html>